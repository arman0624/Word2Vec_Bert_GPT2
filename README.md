# Word2Vec, BERT, and GPT2 Implementation

This implementation focuses on building vector representations (or embeddings) of words. I explored two strategies for this task, one where I trained the vector representations from scratch given a large corpus of text and another where extract representations from large pretrained language models. In either case, the goal is to produce vector representations of each of the words in a given text in such a way that the dot product of these vectors accurately represents the semantic similarity of the words they represent. The representations are evaluated on two separate datasets, one where word pairs are provided in isolation and another where the word pairs occur in a shared context. The evaluation code calculates the similarity of a word pair by taking the dot product of the two words’ vectors. The entire dataset is then scored using Spearman’s rank correlation coefficient between these dot products and human annotations, separately for each dataset.
